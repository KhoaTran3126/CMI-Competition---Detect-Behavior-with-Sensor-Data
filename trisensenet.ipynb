{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744e4cc1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:24.273514Z",
     "iopub.status.busy": "2025-06-21T03:25:24.273107Z",
     "iopub.status.idle": "2025-06-21T03:25:32.345193Z",
     "shell.execute_reply": "2025-06-21T03:25:32.344595Z"
    },
    "papermill": {
     "duration": 8.079764,
     "end_time": "2025-06-21T03:25:32.346636",
     "exception": false,
     "start_time": "2025-06-21T03:25:24.266872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dc3422",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:32.357017Z",
     "iopub.status.busy": "2025-06-21T03:25:32.356305Z",
     "iopub.status.idle": "2025-06-21T03:25:32.439445Z",
     "shell.execute_reply": "2025-06-21T03:25:32.438817Z"
    },
    "papermill": {
     "duration": 0.089316,
     "end_time": "2025-06-21T03:25:32.440716",
     "exception": false,
     "start_time": "2025-06-21T03:25:32.351400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMU_LEN = 14\n",
    "THM_TOF_SUMMARIES_LEN = 30\n",
    "BATCH_SIZE = 64\n",
    "PAD_PERCENTILE = 95\n",
    "PAD_LEN = 127\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "MIXUP_ALPHA = 0.4\n",
    "EPOCHS = 200\n",
    "PATIENCE = 15\n",
    "SEED = 3126\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92031a77",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:32.450310Z",
     "iopub.status.busy": "2025-06-21T03:25:32.450075Z",
     "iopub.status.idle": "2025-06-21T03:25:32.542115Z",
     "shell.execute_reply": "2025-06-21T03:25:32.541395Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.098393,
     "end_time": "2025-06-21T03:25:32.543603",
     "exception": false,
     "start_time": "2025-06-21T03:25:32.445210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    \"\"\"Errors raised here will be shown directly to the competitor.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class CompetitionMetric:\n",
    "    \"\"\"Hierarchical macro F1 for the CMI 2025 challenge.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.target_gestures = [\n",
    "            'Above ear - pull hair',\n",
    "            'Cheek - pinch skin',\n",
    "            'Eyebrow - pull hair',\n",
    "            'Eyelash - pull hair',\n",
    "            'Forehead - pull hairline',\n",
    "            'Forehead - scratch',\n",
    "            'Neck - pinch skin',\n",
    "            'Neck - scratch',\n",
    "        ]\n",
    "        self.non_target_gestures = [\n",
    "            'Write name on leg',\n",
    "            'Wave hello',\n",
    "            'Glasses on/off',\n",
    "            'Text on phone',\n",
    "            'Write name in air',\n",
    "            'Feel around in tray and pull out an object',\n",
    "            'Scratch knee/leg skin',\n",
    "            'Pull air toward your face',\n",
    "            'Drink from bottle/cup',\n",
    "            'Pinch knee/leg skin'\n",
    "        ]\n",
    "        self.all_classes = self.target_gestures + self.non_target_gestures\n",
    "\n",
    "    def calculate_hierarchical_f1(\n",
    "        self,\n",
    "        sol: pd.DataFrame,\n",
    "        sub: pd.DataFrame\n",
    "    ) -> float:\n",
    "\n",
    "        # Validate gestures\n",
    "        invalid_types = {i for i in sub['gesture'].unique() if i not in self.all_classes}\n",
    "        if invalid_types:\n",
    "            raise ParticipantVisibleError(\n",
    "                f\"Invalid gesture values in submission: {invalid_types}\"\n",
    "            )\n",
    "\n",
    "        # Compute binary F1 (Target vs Non-Target)\n",
    "        y_true_bin = sol['gesture'].isin(self.target_gestures).values\n",
    "        y_pred_bin = sub['gesture'].isin(self.target_gestures).values\n",
    "        \n",
    "        f1_binary = f1_score(y_true_bin, y_pred_bin, pos_label=True, zero_division=0, average='binary')\n",
    "\n",
    "        # Build multi-class labels for gestures\n",
    "        y_true_mc = sol['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
    "        y_pred_mc = sub['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
    "\n",
    "        f1_macro = f1_score(y_true_mc, y_pred_mc, average='macro', zero_division=0)\n",
    "\n",
    "        return f1_binary, f1_macro, (f1_binary+f1_macro)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbfa95eb",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:32.553268Z",
     "iopub.status.busy": "2025-06-21T03:25:32.552824Z",
     "iopub.status.idle": "2025-06-21T03:25:32.557679Z",
     "shell.execute_reply": "2025-06-21T03:25:32.556990Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.010754,
     "end_time": "2025-06-21T03:25:32.558804",
     "exception": false,
     "start_time": "2025-06-21T03:25:32.548050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def F1_score(y_val, y_pred, lbl_encoder, choice=\"weighted\"):\n",
    "    metric = CompetitionMetric()\n",
    "    y_val  = pd.DataFrame({'id':range(len(y_val)), \n",
    "                           'gesture':y_val})\n",
    "    y_pred = pd.DataFrame({'id':range(len(y_pred)), \n",
    "                           'gesture':y_pred})\n",
    "\n",
    "    ## Convert numeric labels to original descriptions\n",
    "    y_val[\"gesture\"]  = lbl_encoder.inverse_transform(y_val[\"gesture\"])\n",
    "    y_pred[\"gesture\"] = lbl_encoder.inverse_transform(y_pred[\"gesture\"])\n",
    "\n",
    "    ## Computes score\n",
    "    binary, macro, weighted = metric.calculate_hierarchical_f1(y_val, y_pred)\n",
    "\n",
    "    ## Returns result\n",
    "    if choice==\"binary\": return binary\n",
    "    elif choice==\"macro\": return macro\n",
    "    elif choice==\"weighted\": return weighted\n",
    "    else: return (binary, macro, weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8709e35c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:32.568110Z",
     "iopub.status.busy": "2025-06-21T03:25:32.567478Z",
     "iopub.status.idle": "2025-06-21T03:25:32.576523Z",
     "shell.execute_reply": "2025-06-21T03:25:32.575847Z"
    },
    "papermill": {
     "duration": 0.014767,
     "end_time": "2025-06-21T03:25:32.577672",
     "exception": false,
     "start_time": "2025-06-21T03:25:32.562905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_all(seed=3126):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "\n",
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da13a91d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:32.586536Z",
     "iopub.status.busy": "2025-06-21T03:25:32.586309Z",
     "iopub.status.idle": "2025-06-21T03:25:39.203621Z",
     "shell.execute_reply": "2025-06-21T03:25:39.202826Z"
    },
    "papermill": {
     "duration": 6.623347,
     "end_time": "2025-06-21T03:25:39.205078",
     "exception": false,
     "start_time": "2025-06-21T03:25:32.581731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAW_DIR = \"/kaggle/input/cmi-detect-behavior-with-sensor-data\"\n",
    "\n",
    "label_encoder = joblib.load(\"/kaggle/input/cmi-label-encoder/label_encoder.joblib\")\n",
    "standard_scaler = joblib.load(\"/kaggle/input/cmi-custom-tensor-data-v2/StandardScaler.joblib\")\n",
    "X = torch.load(\"/kaggle/input/cmi-custom-tensor-data-v2/X.pt\")\n",
    "y_int = np.load(\"/kaggle/input/cmi-custom-tensor-data-v2/y_int.npy\")\n",
    "y_ohe = torch.load(\"/kaggle/input/cmi-custom-tensor-data-v2/y_ohe.pt\")\n",
    "\n",
    "imu_cols = joblib.load(\"/kaggle/input/cmi-custom-tensor-data-v2/imu_cols.joblib\")\n",
    "thm_tof_cols = joblib.load(\"/kaggle/input/cmi-custom-tensor-data-v2/thm_tof_cols.joblib\")\n",
    "final_feature_cols = joblib.load(\"/kaggle/input/cmi-custom-tensor-data-v2/final_feature_cols.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc6b8b8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:39.214953Z",
     "iopub.status.busy": "2025-06-21T03:25:39.214491Z",
     "iopub.status.idle": "2025-06-21T03:25:39.218884Z",
     "shell.execute_reply": "2025-06-21T03:25:39.218126Z"
    },
    "papermill": {
     "duration": 0.010278,
     "end_time": "2025-06-21T03:25:39.219900",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.209622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMU Features:\n",
      "['acc_x', 'acc_y', 'acc_z', 'acc_x_diff', 'acc_y_diff', 'acc_z_diff', 'rot_w', 'rot_x', 'rot_y', 'rot_z', 'acc_mag', 'rot_angle', 'acc_mag_diff', 'rot_angle_diff']\n",
      "\n",
      "\n",
      "Thm+TOF Features:\n",
      "['thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5', 'thm_1_diff', 'thm_2_diff', 'thm_3_diff', 'thm_4_diff', 'thm_5_diff', 'tof_1_mean', 'tof_1_std', 'tof_1_min', 'tof_1_max', 'tof_2_mean', 'tof_2_std', 'tof_2_min', 'tof_2_max', 'tof_3_mean', 'tof_3_std', 'tof_3_min', 'tof_3_max', 'tof_4_mean', 'tof_4_std', 'tof_4_min', 'tof_4_max', 'tof_5_mean', 'tof_5_std', 'tof_5_min', 'tof_5_max', 'tof_1_v0', 'tof_1_v1', 'tof_1_v2', 'tof_1_v3', 'tof_1_v4', 'tof_1_v5', 'tof_1_v6', 'tof_1_v7', 'tof_1_v8', 'tof_1_v9', 'tof_1_v10', 'tof_1_v11', 'tof_1_v12', 'tof_1_v13', 'tof_1_v14', 'tof_1_v15', 'tof_1_v16', 'tof_1_v17', 'tof_1_v18', 'tof_1_v19', 'tof_1_v20', 'tof_1_v21', 'tof_1_v22', 'tof_1_v23', 'tof_1_v24', 'tof_1_v25', 'tof_1_v26', 'tof_1_v27', 'tof_1_v28', 'tof_1_v29', 'tof_1_v30', 'tof_1_v31', 'tof_1_v32', 'tof_1_v33', 'tof_1_v34', 'tof_1_v35', 'tof_1_v36', 'tof_1_v37', 'tof_1_v38', 'tof_1_v39', 'tof_1_v40', 'tof_1_v41', 'tof_1_v42', 'tof_1_v43', 'tof_1_v44', 'tof_1_v45', 'tof_1_v46', 'tof_1_v47', 'tof_1_v48', 'tof_1_v49', 'tof_1_v50', 'tof_1_v51', 'tof_1_v52', 'tof_1_v53', 'tof_1_v54', 'tof_1_v55', 'tof_1_v56', 'tof_1_v57', 'tof_1_v58', 'tof_1_v59', 'tof_1_v60', 'tof_1_v61', 'tof_1_v62', 'tof_1_v63', 'tof_2_v0', 'tof_2_v1', 'tof_2_v2', 'tof_2_v3', 'tof_2_v4', 'tof_2_v5', 'tof_2_v6', 'tof_2_v7', 'tof_2_v8', 'tof_2_v9', 'tof_2_v10', 'tof_2_v11', 'tof_2_v12', 'tof_2_v13', 'tof_2_v14', 'tof_2_v15', 'tof_2_v16', 'tof_2_v17', 'tof_2_v18', 'tof_2_v19', 'tof_2_v20', 'tof_2_v21', 'tof_2_v22', 'tof_2_v23', 'tof_2_v24', 'tof_2_v25', 'tof_2_v26', 'tof_2_v27', 'tof_2_v28', 'tof_2_v29', 'tof_2_v30', 'tof_2_v31', 'tof_2_v32', 'tof_2_v33', 'tof_2_v34', 'tof_2_v35', 'tof_2_v36', 'tof_2_v37', 'tof_2_v38', 'tof_2_v39', 'tof_2_v40', 'tof_2_v41', 'tof_2_v42', 'tof_2_v43', 'tof_2_v44', 'tof_2_v45', 'tof_2_v46', 'tof_2_v47', 'tof_2_v48', 'tof_2_v49', 'tof_2_v50', 'tof_2_v51', 'tof_2_v52', 'tof_2_v53', 'tof_2_v54', 'tof_2_v55', 'tof_2_v56', 'tof_2_v57', 'tof_2_v58', 'tof_2_v59', 'tof_2_v60', 'tof_2_v61', 'tof_2_v62', 'tof_2_v63', 'tof_3_v0', 'tof_3_v1', 'tof_3_v2', 'tof_3_v3', 'tof_3_v4', 'tof_3_v5', 'tof_3_v6', 'tof_3_v7', 'tof_3_v8', 'tof_3_v9', 'tof_3_v10', 'tof_3_v11', 'tof_3_v12', 'tof_3_v13', 'tof_3_v14', 'tof_3_v15', 'tof_3_v16', 'tof_3_v17', 'tof_3_v18', 'tof_3_v19', 'tof_3_v20', 'tof_3_v21', 'tof_3_v22', 'tof_3_v23', 'tof_3_v24', 'tof_3_v25', 'tof_3_v26', 'tof_3_v27', 'tof_3_v28', 'tof_3_v29', 'tof_3_v30', 'tof_3_v31', 'tof_3_v32', 'tof_3_v33', 'tof_3_v34', 'tof_3_v35', 'tof_3_v36', 'tof_3_v37', 'tof_3_v38', 'tof_3_v39', 'tof_3_v40', 'tof_3_v41', 'tof_3_v42', 'tof_3_v43', 'tof_3_v44', 'tof_3_v45', 'tof_3_v46', 'tof_3_v47', 'tof_3_v48', 'tof_3_v49', 'tof_3_v50', 'tof_3_v51', 'tof_3_v52', 'tof_3_v53', 'tof_3_v54', 'tof_3_v55', 'tof_3_v56', 'tof_3_v57', 'tof_3_v58', 'tof_3_v59', 'tof_3_v60', 'tof_3_v61', 'tof_3_v62', 'tof_3_v63', 'tof_4_v0', 'tof_4_v1', 'tof_4_v2', 'tof_4_v3', 'tof_4_v4', 'tof_4_v5', 'tof_4_v6', 'tof_4_v7', 'tof_4_v8', 'tof_4_v9', 'tof_4_v10', 'tof_4_v11', 'tof_4_v12', 'tof_4_v13', 'tof_4_v14', 'tof_4_v15', 'tof_4_v16', 'tof_4_v17', 'tof_4_v18', 'tof_4_v19', 'tof_4_v20', 'tof_4_v21', 'tof_4_v22', 'tof_4_v23', 'tof_4_v24', 'tof_4_v25', 'tof_4_v26', 'tof_4_v27', 'tof_4_v28', 'tof_4_v29', 'tof_4_v30', 'tof_4_v31', 'tof_4_v32', 'tof_4_v33', 'tof_4_v34', 'tof_4_v35', 'tof_4_v36', 'tof_4_v37', 'tof_4_v38', 'tof_4_v39', 'tof_4_v40', 'tof_4_v41', 'tof_4_v42', 'tof_4_v43', 'tof_4_v44', 'tof_4_v45', 'tof_4_v46', 'tof_4_v47', 'tof_4_v48', 'tof_4_v49', 'tof_4_v50', 'tof_4_v51', 'tof_4_v52', 'tof_4_v53', 'tof_4_v54', 'tof_4_v55', 'tof_4_v56', 'tof_4_v57', 'tof_4_v58', 'tof_4_v59', 'tof_4_v60', 'tof_4_v61', 'tof_4_v62', 'tof_4_v63', 'tof_5_v0', 'tof_5_v1', 'tof_5_v2', 'tof_5_v3', 'tof_5_v4', 'tof_5_v5', 'tof_5_v6', 'tof_5_v7', 'tof_5_v8', 'tof_5_v9', 'tof_5_v10', 'tof_5_v11', 'tof_5_v12', 'tof_5_v13', 'tof_5_v14', 'tof_5_v15', 'tof_5_v16', 'tof_5_v17', 'tof_5_v18', 'tof_5_v19', 'tof_5_v20', 'tof_5_v21', 'tof_5_v22', 'tof_5_v23', 'tof_5_v24', 'tof_5_v25', 'tof_5_v26', 'tof_5_v27', 'tof_5_v28', 'tof_5_v29', 'tof_5_v30', 'tof_5_v31', 'tof_5_v32', 'tof_5_v33', 'tof_5_v34', 'tof_5_v35', 'tof_5_v36', 'tof_5_v37', 'tof_5_v38', 'tof_5_v39', 'tof_5_v40', 'tof_5_v41', 'tof_5_v42', 'tof_5_v43', 'tof_5_v44', 'tof_5_v45', 'tof_5_v46', 'tof_5_v47', 'tof_5_v48', 'tof_5_v49', 'tof_5_v50', 'tof_5_v51', 'tof_5_v52', 'tof_5_v53', 'tof_5_v54', 'tof_5_v55', 'tof_5_v56', 'tof_5_v57', 'tof_5_v58', 'tof_5_v59', 'tof_5_v60', 'tof_5_v61', 'tof_5_v62', 'tof_5_v63']\n"
     ]
    }
   ],
   "source": [
    "print(\"IMU Features:\")\n",
    "print(imu_cols)\n",
    "print(\"\\n\\nThm+TOF Features:\")\n",
    "print(thm_tof_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab54b9",
   "metadata": {
    "papermill": {
     "duration": 0.004093,
     "end_time": "2025-06-21T03:25:39.228272",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.224179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9319808b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:39.237826Z",
     "iopub.status.busy": "2025-06-21T03:25:39.237216Z",
     "iopub.status.idle": "2025-06-21T03:25:39.243643Z",
     "shell.execute_reply": "2025-06-21T03:25:39.243098Z"
    },
    "papermill": {
     "duration": 0.01217,
     "end_time": "2025-06-21T03:25:39.244646",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.232476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualCoordinateAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Coordinate Attention adapted for 1D temporal sequences.\n",
    "    Input: (B, T, C)\n",
    "    Output: (B, T, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super(ResidualCoordinateAttention, self).__init__()\n",
    "        self.mid_channels = max(8, channels // reduction)\n",
    "        self.pos_embed_T = nn.Parameter(torch.randn(1, self.mid_channels, PAD_LEN))\n",
    "        self.gamma = nn.Parameter(torch.tensor(1.0))  # learnable scaling\n",
    "\n",
    "        # Temporal pooling to preserve time-dimension structure\n",
    "        self.conv1 = nn.Conv1d(channels, self.mid_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(self.mid_channels)\n",
    "        self.act = nn.SiLU()\n",
    "        \n",
    "        # Learn attention over time (coordinate)\n",
    "        self.attn_T = nn.Conv1d(self.mid_channels, 1, kernel_size=1, bias=False)\n",
    "        self.attn_C = nn.Conv1d(self.mid_channels, channels, kernel_size=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, T, C) â†’ (B, C, T)\n",
    "        x_perm = x.permute(0, 2, 1)\n",
    "        # (B, rC, T)\n",
    "        f = self.act(self.bn1(self.conv1(x_perm))) \n",
    "        f = f + self.pos_embed_T[:, :, :f.shape[-1]]\n",
    "        # (B, 1, T)\n",
    "        time_attn = self.sigmoid(self.attn_T(f)) \n",
    "        # (B, C, 1)\n",
    "        channel_attn = self.sigmoid(self.attn_C(f.mean(dim=2, keepdim=True))) \n",
    "        \n",
    "        out = x_perm + self.gamma * (x_perm * time_attn * channel_attn)  # (B, C, T)\n",
    "        return out.permute(0, 2, 1)  # Back to (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a5efe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:39.254260Z",
     "iopub.status.busy": "2025-06-21T03:25:39.254082Z",
     "iopub.status.idle": "2025-06-21T03:25:39.260854Z",
     "shell.execute_reply": "2025-06-21T03:25:39.260340Z"
    },
    "papermill": {
     "duration": 0.012302,
     "end_time": "2025-06-21T03:25:39.261783",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.249481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualCNNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual CNN Block with Squeeze-and-Excitation (SE)\n",
    "    Input expected in (batch_size, timesteps, channels) format.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, pool_size=2, drop=0.3):\n",
    "        super(ResidualCNNBlock, self).__init__()\n",
    "        # PyTorch Conv1D expects (batch_size, channels, timesteps)\n",
    "\n",
    "        ## CNN model\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, padding='same', bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size, padding='same', bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.attention = ResidualCoordinateAttention(out_channels)\n",
    "\n",
    "        self.shortcut_proj = None\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut_proj = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, padding='same', bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "        self.relu_final = nn.ReLU(inplace=True)\n",
    "        if pool_size is not None: \n",
    "            self.max_pool = nn.MaxPool1d(pool_size)\n",
    "        else: \n",
    "            self.max_pool = None\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x                                      # (B, T, C_in)\n",
    "        x_permuted = self.cnn(x.permute(0, 2, 1))         # (B, C_out, T)\n",
    "        x_attn = self.attention(x_permuted.permute(0, 2, 1)) # (B, T, C_out)\n",
    "\n",
    "        # Handle shortcut connection\n",
    "        if self.shortcut_proj:\n",
    "            shortcut = self.shortcut_proj(shortcut.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        # Residual connection\n",
    "        x = self.relu_final(x_attn + shortcut)\n",
    "        if self.max_pool is not None:\n",
    "            x = self.max_pool(x.permute(0, 2, 1)).permute(0, 2, 1) # (B, T, C_out) -> (B, T//pool_size, C_out)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b55f67c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:39.270686Z",
     "iopub.status.busy": "2025-06-21T03:25:39.270508Z",
     "iopub.status.idle": "2025-06-21T03:25:39.274828Z",
     "shell.execute_reply": "2025-06-21T03:25:39.274312Z"
    },
    "papermill": {
     "duration": 0.009944,
     "end_time": "2025-06-21T03:25:39.275859",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.265915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TOFCompressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TOFCompressor, self).__init__()\n",
    "        self.sensor_cnn = nn.Sequential(\n",
    "            nn.Conv2d(5, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        x = x.view(B*T, 5, 8, 8)  # (B,T,320) --> (B*T, 5, 8, 8)\n",
    "        x = self.sensor_cnn(x)  # (B*T, 32, 1, 1)\n",
    "        x = x.view(B, T, 32) # (B, T, 32)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa45a7a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:39.284909Z",
     "iopub.status.busy": "2025-06-21T03:25:39.284712Z",
     "iopub.status.idle": "2025-06-21T03:25:39.289471Z",
     "shell.execute_reply": "2025-06-21T03:25:39.288767Z"
    },
    "papermill": {
     "duration": 0.010426,
     "end_time": "2025-06-21T03:25:39.290507",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.280081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism to weigh the importance of different timesteps.\n",
    "    Input expected in (batch_size, timesteps, features) format.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(MLPAttention, self).__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim//8),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(feature_dim//8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs shape: (B, T, C)\n",
    "        score = self.attn(inputs).squeeze(-1) # (B, T)\n",
    "        weights = F.softmax(score, dim=-1).unsqueeze(-1) # (B, T, 1)\n",
    "        context = (inputs * weights).sum(dim=1) # (B, T, C) -> (B, C)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a1f71b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:39.299766Z",
     "iopub.status.busy": "2025-06-21T03:25:39.299557Z",
     "iopub.status.idle": "2025-06-21T03:25:39.310754Z",
     "shell.execute_reply": "2025-06-21T03:25:39.310220Z"
    },
    "papermill": {
     "duration": 0.017092,
     "end_time": "2025-06-21T03:25:39.311740",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.294648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TriSenseNet(nn.Module):\n",
    "    def __init__(self, pad_len, imu_dim, thm_tof_summaries_dim, n_classes=18):\n",
    "        super(TriSenseNet, self).__init__()\n",
    "        self.imu_dim = imu_dim\n",
    "        self.thm_tof_summaries_dim = thm_tof_summaries_dim\n",
    "        \n",
    "        # --- IMU Branch ---\n",
    "        # (B, T, IMU dim) --> --> (B, T/4, 128)\n",
    "        self.imu_branch = nn.Sequential(\n",
    "            ResidualCNNBlock(imu_dim, 64, kernel_size=3, pool_size=2, drop=0.1), # Output shape: (B, T/2, 64)\n",
    "            ResidualCNNBlock(64, 64, kernel_size=3, pool_size=None, drop=0.1), # Output shape: (B, T/2, 64)\n",
    "            ResidualCNNBlock(64, 128, kernel_size=5, pool_size=2, drop=0.1) # Output shape: (B, T/4, 128)\n",
    "        )\n",
    "\n",
    "        # --- Thm/TOF Branch ---\n",
    "        # (B, T, Thm+TOF summaries dim) --> (B, T/4, 128)\n",
    "        self.thm_tof_branch = nn.Sequential(\n",
    "            nn.Conv1d(thm_tof_summaries_dim, 64, 3, padding='same', bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv1d(64, 64, 3, padding='same', bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, 3, padding='same', bias=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2)  \n",
    "        )\n",
    "\n",
    "        # --- Raw TOF Branch ---\n",
    "        # (B, T, TOF raw dim) --> (B, T/4, 128)\n",
    "        self.tof_raw_branch = nn.Sequential(\n",
    "            TOFCompressor(), # Output shape: (B, T, 32)\n",
    "            ResidualCNNBlock(32, 64, kernel_size=3, pool_size=2, drop=0.1), # Output shape: (B, T/2, 64)\n",
    "            ResidualCNNBlock(64, 64, kernel_size=3, pool_size=None, drop=0.1), # Output shape: (B, T/2, 64)\n",
    "            ResidualCNNBlock(64, 128, kernel_size=5, pool_size=2, drop=0.1) # Output shape: (B, T/4, 128)\n",
    "        )\n",
    "\n",
    "\n",
    "        # --- Merged Branch and Recurrent Layers ---\n",
    "        # Merged dimension: 128 (IMU) + 128 (Thm + TOF summaries) + 128 (TOF raw features) = 384\n",
    "        merged_feature_dim = 128 + 128 + 128\n",
    "        self.lstm = nn.LSTM(merged_feature_dim, hidden_size=128, bidirectional=True, batch_first=True)\n",
    "        self.gru  = nn.GRU(merged_feature_dim, hidden_size=128, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Output of bidirectional LSTM/GRU will be 2 * hidden_size\n",
    "        # (batch_size, timesteps_after_pooling, 2 * 128) = (batch_size, pad_len/4, 256)\n",
    "\n",
    "        # For x_merged path\n",
    "        self.gaussian_noise_std = 0.09 \n",
    "        self.dense = nn.Linear(merged_feature_dim, 16)\n",
    "        self.elu   = nn.ELU()\n",
    "        \n",
    "        # Concatenated features \n",
    "        # x_gru: (B, T, 256)\n",
    "        # x_lstm: (B, T, 256)\n",
    "        # x_merged: (B, T, 16)\n",
    "        self.concat_dropout = nn.Dropout(0.4)\n",
    "        self.attention_layer = MLPAttention(528)\n",
    "\n",
    "        # --- Classification Head ---\n",
    "        # After attention, shape is (B, 512)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(528, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(256, 128, bias=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(128, n_classes)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x_imu     = inp[:, :, :self.imu_dim] # (B, T, IMU dim)\n",
    "        x_thm_tof = inp[:, :, self.imu_dim : self.thm_tof_summaries_dim+self.imu_dim] # (B, T, Thm + TOF summaries dim)\n",
    "        x_tof_raw = inp[:, :, self.thm_tof_summaries_dim+self.imu_dim :] # (B, T, TOF raw dim)\n",
    "        \n",
    "        # --- IMU Branch ---\n",
    "        x_imu = self.imu_branch(x_imu) # (B, T/4, 128)\n",
    "\n",
    "        # --- Thm/TOF summaries Branch ---\n",
    "        x_thm_tof = self.thm_tof_branch(x_thm_tof.permute(0, 2, 1)) \n",
    "        x_thm_tof = x_thm_tof.permute(0, 2, 1) # (B, T/4, 128)\n",
    "\n",
    "        # --- TOF raw Branch ---\n",
    "        x_tof_raw = self.tof_raw_branch(x_tof_raw) # (B, T/4, 128)\n",
    "        \n",
    "        # --- Merge Branches ---\n",
    "        merged = torch.cat([x_imu, x_thm_tof, x_tof_raw], dim=-1) # (B, T/4, 384)\n",
    "\n",
    "        # --- Recurrent Layers ---\n",
    "        x_lstm, _ = self.lstm(merged) # (B, T/4, 256)\n",
    "        x_gru, _  = self.gru(merged)  # (B, T/4, 256)\n",
    "        \n",
    "        # x_merged path (gaussian noise)\n",
    "        if self.training: \n",
    "            x_merged = merged + torch.randn_like(merged)*self.gaussian_noise_std\n",
    "        else:\n",
    "            x_merged = merged\n",
    "        x_merged = self.elu(self.dense(x_merged)) # (B, T/4, 16)\n",
    "\n",
    "        # Concatenate outputs of all three paths\n",
    "        x = torch.cat([x_lstm, x_gru, x_merged], dim=-1) # (B, T/4, 384*2 + 16) = (B, T/4, 528)\n",
    "        x = self.concat_dropout(x)\n",
    "\n",
    "        # Attention layer\n",
    "        x = self.attention_layer(x) # Output: (B, 528)\n",
    "\n",
    "        # --- Classification Head ---\n",
    "        x = self.classifier(x)\n",
    "        out = self.output_layer(x) # (B, 18)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3cfb975",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:39.320792Z",
     "iopub.status.busy": "2025-06-21T03:25:39.320592Z",
     "iopub.status.idle": "2025-06-21T03:25:39.325543Z",
     "shell.execute_reply": "2025-06-21T03:25:39.324998Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.010534,
     "end_time": "2025-06-21T03:25:39.326557",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.316023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_model_weights(model:nn.Module):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            nn.init.kaiming_uniform_(module.weight, nonlinearity=\"relu\")\n",
    "            \n",
    "        elif isinstance(module, (nn.LSTM, nn.GRU)):\n",
    "            for name, param in module.named_parameters():\n",
    "                if 'weight_ih' in name: \n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name: \n",
    "                    nn.init.orthogonal_(param.data) \n",
    "                elif 'bias_ih' in name or 'bias_hh' in name: \n",
    "                    nn.init.constant_(param.data, 0)\n",
    "                    if 'bias_ih' in name and isinstance(module, nn.LSTM):\n",
    "                        nn.init.constant_(param.data[module.hidden_size : 2 * module.hidden_size], 1.0)\n",
    "        \n",
    "        elif isinstance(module, nn.BatchNorm1d):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36532579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:39.335488Z",
     "iopub.status.busy": "2025-06-21T03:25:39.335272Z",
     "iopub.status.idle": "2025-06-21T03:25:39.340564Z",
     "shell.execute_reply": "2025-06-21T03:25:39.340049Z"
    },
    "papermill": {
     "duration": 0.010909,
     "end_time": "2025-06-21T03:25:39.341597",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.330688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MixupDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, alpha: float = 0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (np.ndarray): Features (e.g., padded time series data).\n",
    "                            Expected shape (num_samples, timesteps, features).\n",
    "            y (np.ndarray): Labels (e.g., one-hot encoded or class indices).\n",
    "                            Expected shape (num_samples, num_classes) for one-hot,\n",
    "                            or (num_samples,) for class indices.\n",
    "            alpha (float): Alpha parameter for the Beta distribution used in Mixup.\n",
    "        \"\"\"\n",
    "        # Convert X and y to PyTorch tensors once\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32 if (y.ndim>1) else (torch.long)) # Use long for class indices\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data for Mixup.\n",
    "        \"\"\"\n",
    "        x, y = self.X[idx], self.y[idx]\n",
    "        \n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "            rand_idx = np.random.randint(0, len(self.X))\n",
    "            x_rand, y_rand = self.X[rand_idx], self.y[rand_idx]\n",
    "            \n",
    "            x = lam * x + (1 - lam) * x_rand\n",
    "            y = lam * y + (1 - lam) * y_rand\n",
    "            \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0c2d6ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:39.350443Z",
     "iopub.status.busy": "2025-06-21T03:25:39.350211Z",
     "iopub.status.idle": "2025-06-21T03:25:39.353895Z",
     "shell.execute_reply": "2025-06-21T03:25:39.353405Z"
    },
    "papermill": {
     "duration": 0.009267,
     "end_time": "2025-06-21T03:25:39.354956",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.345689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def label_smoothing_loss(pred, target, smoothing=0.1):\n",
    "    \"\"\"Label smoothing loss\"\"\"\n",
    "    confidence = 1.0 - smoothing\n",
    "    log_probs = F.log_softmax(pred, dim=-1)\n",
    "    nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "    nll_loss = nll_loss.squeeze(1)\n",
    "    smooth_loss = -log_probs.mean(dim=-1)\n",
    "    loss = confidence * nll_loss + smoothing * smooth_loss\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57403691",
   "metadata": {
    "papermill": {
     "duration": 0.003875,
     "end_time": "2025-06-21T03:25:39.362885",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.359010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a90d76df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:39.371697Z",
     "iopub.status.busy": "2025-06-21T03:25:39.371488Z",
     "iopub.status.idle": "2025-06-21T03:25:39.390011Z",
     "shell.execute_reply": "2025-06-21T03:25:39.389539Z"
    },
    "papermill": {
     "duration": 0.024068,
     "end_time": "2025-06-21T03:25:39.391038",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.366970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dbe8b20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:39.399998Z",
     "iopub.status.busy": "2025-06-21T03:25:39.399795Z",
     "iopub.status.idle": "2025-06-21T03:25:40.683970Z",
     "shell.execute_reply": "2025-06-21T03:25:40.683368Z"
    },
    "papermill": {
     "duration": 1.290022,
     "end_time": "2025-06-21T03:25:40.685223",
     "exception": false,
     "start_time": "2025-06-21T03:25:39.395201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X.numpy(), y_ohe.numpy(), \n",
    "        test_size=0.2, random_state=SEED, stratify=y_int\n",
    ")\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "\n",
    "cw_vals = compute_class_weight('balanced', \n",
    "                               classes=np.arange(len(label_encoder.classes_)),\n",
    "                               y=y_int)\n",
    "class_weights = torch.FloatTensor(cw_vals).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bd5de76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:40.694754Z",
     "iopub.status.busy": "2025-06-21T03:25:40.694519Z",
     "iopub.status.idle": "2025-06-21T03:25:41.267851Z",
     "shell.execute_reply": "2025-06-21T03:25:41.267197Z"
    },
    "papermill": {
     "duration": 0.57955,
     "end_time": "2025-06-21T03:25:41.269312",
     "exception": false,
     "start_time": "2025-06-21T03:25:40.689762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_dataset = MixupDataset(X_train, y_train, alpha=MIXUP_ALPHA)\n",
    "val_dataset = MixupDataset(X_val, y_val, alpha=0.0)  \n",
    "    \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, worker_init_fn=worker_init_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a06d0e",
   "metadata": {
    "papermill": {
     "duration": 0.004149,
     "end_time": "2025-06-21T03:25:41.278145",
     "exception": false,
     "start_time": "2025-06-21T03:25:41.273996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6f78426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:41.287884Z",
     "iopub.status.busy": "2025-06-21T03:25:41.287238Z",
     "iopub.status.idle": "2025-06-21T03:25:46.470882Z",
     "shell.execute_reply": "2025-06-21T03:25:46.470304Z"
    },
    "papermill": {
     "duration": 5.189802,
     "end_time": "2025-06-21T03:25:46.472192",
     "exception": false,
     "start_time": "2025-06-21T03:25:41.282390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TriSenseNet(PAD_LEN, IMU_LEN, THM_TOF_SUMMARIES_LEN).to(DEVICE)\n",
    "init_model_weights(model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR_INIT, weight_decay=WD)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5*steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efe81b4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:46.482471Z",
     "iopub.status.busy": "2025-06-21T03:25:46.481730Z",
     "iopub.status.idle": "2025-06-21T03:25:46.486754Z",
     "shell.execute_reply": "2025-06-21T03:25:46.486185Z"
    },
    "papermill": {
     "duration": 0.011015,
     "end_time": "2025-06-21T03:25:46.487810",
     "exception": false,
     "start_time": "2025-06-21T03:25:46.476795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class F1EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_f1 = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, model, current_f1):\n",
    "        if current_f1 > self.best_f1:\n",
    "            self.best_f1 = current_f1\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"F1 EarlyStopping: {self.counter}/{self.patience}\\n\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3e59378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:46.497087Z",
     "iopub.status.busy": "2025-06-21T03:25:46.496696Z",
     "iopub.status.idle": "2025-06-21T03:25:46.502707Z",
     "shell.execute_reply": "2025-06-21T03:25:46.502035Z"
    },
    "papermill": {
     "duration": 0.011691,
     "end_time": "2025-06-21T03:25:46.503744",
     "exception": false,
     "start_time": "2025-06-21T03:25:46.492053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, class_weights, lr_scheduler=None):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "        \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
    "                    \n",
    "        logits = model(batch_x)\n",
    "                    \n",
    "        # Handle mixup targets\n",
    "        if batch_y.ndim == 2 and batch_y.shape[1] > 1:  # MixUp or one-hot\n",
    "            sample_weights = torch.sum(batch_y * class_weights.unsqueeze(0), dim=1)\n",
    "            log_probs = F.log_softmax(logits, dim=1)\n",
    "            loss_vec = -torch.sum(log_probs * batch_y, dim=1)  # (B,)\n",
    "            loss = (loss_vec * sample_weights).mean()\n",
    "            targets = batch_y.argmax(dim=1)\n",
    "        else:\n",
    "            targets = batch_y.long()\n",
    "            loss = label_smoothing_loss(logits, targets, smoothing=0.1)            \n",
    "        \n",
    "        ## Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ## Accmulate loss and accuracy\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "        total_correct += (logits.argmax(dim=1) == targets).sum().item()\n",
    "        total_samples += batch_x.size(0)\n",
    "        \n",
    "    if lr_scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    ## Normalize loss and accuracy\n",
    "    train_loss = total_loss/total_samples\n",
    "    train_acc  = total_correct/total_samples\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec2d7930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:46.512620Z",
     "iopub.status.busy": "2025-06-21T03:25:46.512406Z",
     "iopub.status.idle": "2025-06-21T03:25:46.518336Z",
     "shell.execute_reply": "2025-06-21T03:25:46.517694Z"
    },
    "papermill": {
     "duration": 0.01161,
     "end_time": "2025-06-21T03:25:46.519460",
     "exception": false,
     "start_time": "2025-06-21T03:25:46.507850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, lbl_encoder):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_correct, total_samples = 0, 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
    "            logits = model(batch_x)\n",
    "\n",
    "            # For both loss and metrics, assume batch_y is one-hot or integer labels\n",
    "            if batch_y.ndim == 2:\n",
    "                targets = batch_y.argmax(dim=1)\n",
    "            else:\n",
    "                targets = batch_y\n",
    "\n",
    "            # Computes loss and predictions\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            # Track batch metrics\n",
    "            val_loss += loss.item() * batch_x.size(0)\n",
    "            total_correct += (preds == targets).sum().item()\n",
    "            total_samples += batch_x.size(0)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "\n",
    "    # Stack all predictions and targets\n",
    "    y_pred_all = torch.cat(all_preds).numpy()\n",
    "    y_val_all  = torch.cat(all_targets).numpy()\n",
    "\n",
    "    # Compute custom hierarchical F1\n",
    "    val_f1 = F1_score(y_val_all, y_pred_all, lbl_encoder, choice=\"weighted\")\n",
    "    val_loss = val_loss / total_samples\n",
    "    val_acc  = total_correct / total_samples\n",
    "\n",
    "    return val_loss, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d52c7e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:46.528699Z",
     "iopub.status.busy": "2025-06-21T03:25:46.528255Z",
     "iopub.status.idle": "2025-06-21T03:25:46.533914Z",
     "shell.execute_reply": "2025-06-21T03:25:46.533407Z"
    },
    "papermill": {
     "duration": 0.011248,
     "end_time": "2025-06-21T03:25:46.534908",
     "exception": false,
     "start_time": "2025-06-21T03:25:46.523660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_eval(epochs, model, train_loader, val_loader, optimizer, class_weights, lbl_encoder, lr_scheduler=None):\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    val_F1 = []\n",
    "    history = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        ## Trains model\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, class_weights, lr_scheduler)\n",
    "\n",
    "        ## Evaluates model\n",
    "        val_loss, val_acc, val_f1 = evaluate(model, val_loader, lbl_encoder)\n",
    "\n",
    "        ## Append metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_F1.append(val_f1)\n",
    "\n",
    "        ## Checks early stopping\n",
    "        if early_stopper is not None:\n",
    "            early_stopper(model, val_f1)\n",
    "            if early_stopper.early_stop:\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch+1}\\n\")\n",
    "                break\n",
    "                \n",
    "        ## Displays any result\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        print(f\"Train Accuracy: {train_acc:.3f}     Train Loss: {train_loss:.3f}\")\n",
    "        print(f\"Val Accuracy:   {val_acc:.3f}       Val F1:     {val_f1:.2f}     Val Loss:   {val_loss:.3f}\\n\")\n",
    "\n",
    "    ## Save results to history dictionary\n",
    "    history[\"train_losses\"] = train_losses\n",
    "    history[\"train_accuracies\"] = train_accuracies\n",
    "    history[\"val_losses\"] = val_losses\n",
    "    history[\"val_accuracies\"] = val_accuracies\n",
    "    history[\"val_F1\"] = val_F1\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccd34532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T03:25:46.543745Z",
     "iopub.status.busy": "2025-06-21T03:25:46.543543Z",
     "iopub.status.idle": "2025-06-21T04:25:04.549897Z",
     "shell.execute_reply": "2025-06-21T04:25:04.549000Z"
    },
    "papermill": {
     "duration": 3558.020985,
     "end_time": "2025-06-21T04:25:04.560029",
     "exception": false,
     "start_time": "2025-06-21T03:25:46.539044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200]\n",
      "Train Accuracy: 0.148     Train Loss: 2.823\n",
      "Val Accuracy:   0.324       Val F1:     0.55     Val Loss:   2.132\n",
      "\n",
      "Epoch [2/200]\n",
      "Train Accuracy: 0.257     Train Loss: 2.337\n",
      "Val Accuracy:   0.408       Val F1:     0.62     Val Loss:   1.824\n",
      "\n",
      "Epoch [3/200]\n",
      "Train Accuracy: 0.336     Train Loss: 2.113\n",
      "Val Accuracy:   0.454       Val F1:     0.65     Val Loss:   1.600\n",
      "\n",
      "Epoch [4/200]\n",
      "Train Accuracy: 0.382     Train Loss: 1.965\n",
      "Val Accuracy:   0.515       Val F1:     0.69     Val Loss:   1.427\n",
      "\n",
      "Epoch [5/200]\n",
      "Train Accuracy: 0.434     Train Loss: 1.876\n",
      "Val Accuracy:   0.521       Val F1:     0.70     Val Loss:   1.332\n",
      "\n",
      "Epoch [6/200]\n",
      "Train Accuracy: 0.478     Train Loss: 1.743\n",
      "Val Accuracy:   0.553       Val F1:     0.72     Val Loss:   1.250\n",
      "\n",
      "Epoch [7/200]\n",
      "Train Accuracy: 0.489     Train Loss: 1.724\n",
      "Val Accuracy:   0.571       Val F1:     0.72     Val Loss:   1.223\n",
      "\n",
      "Epoch [8/200]\n",
      "Train Accuracy: 0.517     Train Loss: 1.657\n",
      "Val Accuracy:   0.609       Val F1:     0.75     Val Loss:   1.111\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [9/200]\n",
      "Train Accuracy: 0.537     Train Loss: 1.612\n",
      "Val Accuracy:   0.595       Val F1:     0.75     Val Loss:   1.118\n",
      "\n",
      "Epoch [10/200]\n",
      "Train Accuracy: 0.550     Train Loss: 1.575\n",
      "Val Accuracy:   0.621       Val F1:     0.76     Val Loss:   1.059\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [11/200]\n",
      "Train Accuracy: 0.572     Train Loss: 1.516\n",
      "Val Accuracy:   0.617       Val F1:     0.76     Val Loss:   1.054\n",
      "\n",
      "Epoch [12/200]\n",
      "Train Accuracy: 0.587     Train Loss: 1.498\n",
      "Val Accuracy:   0.633       Val F1:     0.77     Val Loss:   1.019\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [13/200]\n",
      "Train Accuracy: 0.589     Train Loss: 1.466\n",
      "Val Accuracy:   0.626       Val F1:     0.76     Val Loss:   1.036\n",
      "\n",
      "Epoch [14/200]\n",
      "Train Accuracy: 0.607     Train Loss: 1.448\n",
      "Val Accuracy:   0.639       Val F1:     0.78     Val Loss:   0.982\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [15/200]\n",
      "Train Accuracy: 0.616     Train Loss: 1.426\n",
      "Val Accuracy:   0.650       Val F1:     0.78     Val Loss:   0.965\n",
      "\n",
      "F1 EarlyStopping: 2/15\n",
      "\n",
      "Epoch [16/200]\n",
      "Train Accuracy: 0.630     Train Loss: 1.386\n",
      "Val Accuracy:   0.647       Val F1:     0.77     Val Loss:   0.959\n",
      "\n",
      "Epoch [17/200]\n",
      "Train Accuracy: 0.638     Train Loss: 1.389\n",
      "Val Accuracy:   0.650       Val F1:     0.78     Val Loss:   0.931\n",
      "\n",
      "Epoch [18/200]\n",
      "Train Accuracy: 0.635     Train Loss: 1.361\n",
      "Val Accuracy:   0.667       Val F1:     0.79     Val Loss:   0.913\n",
      "\n",
      "Epoch [19/200]\n",
      "Train Accuracy: 0.654     Train Loss: 1.328\n",
      "Val Accuracy:   0.661       Val F1:     0.79     Val Loss:   0.909\n",
      "\n",
      "Epoch [20/200]\n",
      "Train Accuracy: 0.667     Train Loss: 1.323\n",
      "Val Accuracy:   0.675       Val F1:     0.79     Val Loss:   0.889\n",
      "\n",
      "Epoch [21/200]\n",
      "Train Accuracy: 0.665     Train Loss: 1.304\n",
      "Val Accuracy:   0.674       Val F1:     0.80     Val Loss:   0.884\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [22/200]\n",
      "Train Accuracy: 0.678     Train Loss: 1.294\n",
      "Val Accuracy:   0.663       Val F1:     0.79     Val Loss:   0.905\n",
      "\n",
      "Epoch [23/200]\n",
      "Train Accuracy: 0.678     Train Loss: 1.288\n",
      "Val Accuracy:   0.681       Val F1:     0.80     Val Loss:   0.864\n",
      "\n",
      "Epoch [24/200]\n",
      "Train Accuracy: 0.687     Train Loss: 1.281\n",
      "Val Accuracy:   0.682       Val F1:     0.80     Val Loss:   0.873\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [25/200]\n",
      "Train Accuracy: 0.690     Train Loss: 1.241\n",
      "Val Accuracy:   0.687       Val F1:     0.80     Val Loss:   0.835\n",
      "\n",
      "F1 EarlyStopping: 2/15\n",
      "\n",
      "Epoch [26/200]\n",
      "Train Accuracy: 0.689     Train Loss: 1.251\n",
      "Val Accuracy:   0.688       Val F1:     0.80     Val Loss:   0.848\n",
      "\n",
      "Epoch [27/200]\n",
      "Train Accuracy: 0.710     Train Loss: 1.204\n",
      "Val Accuracy:   0.695       Val F1:     0.81     Val Loss:   0.839\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [28/200]\n",
      "Train Accuracy: 0.700     Train Loss: 1.221\n",
      "Val Accuracy:   0.683       Val F1:     0.80     Val Loss:   0.848\n",
      "\n",
      "Epoch [29/200]\n",
      "Train Accuracy: 0.717     Train Loss: 1.205\n",
      "Val Accuracy:   0.700       Val F1:     0.81     Val Loss:   0.831\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [30/200]\n",
      "Train Accuracy: 0.724     Train Loss: 1.192\n",
      "Val Accuracy:   0.684       Val F1:     0.80     Val Loss:   0.859\n",
      "\n",
      "F1 EarlyStopping: 2/15\n",
      "\n",
      "Epoch [31/200]\n",
      "Train Accuracy: 0.732     Train Loss: 1.184\n",
      "Val Accuracy:   0.701       Val F1:     0.81     Val Loss:   0.854\n",
      "\n",
      "Epoch [32/200]\n",
      "Train Accuracy: 0.735     Train Loss: 1.147\n",
      "Val Accuracy:   0.706       Val F1:     0.81     Val Loss:   0.820\n",
      "\n",
      "Epoch [33/200]\n",
      "Train Accuracy: 0.733     Train Loss: 1.161\n",
      "Val Accuracy:   0.704       Val F1:     0.81     Val Loss:   0.822\n",
      "\n",
      "Epoch [34/200]\n",
      "Train Accuracy: 0.747     Train Loss: 1.139\n",
      "Val Accuracy:   0.709       Val F1:     0.82     Val Loss:   0.838\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [35/200]\n",
      "Train Accuracy: 0.749     Train Loss: 1.135\n",
      "Val Accuracy:   0.692       Val F1:     0.81     Val Loss:   0.813\n",
      "\n",
      "F1 EarlyStopping: 2/15\n",
      "\n",
      "Epoch [36/200]\n",
      "Train Accuracy: 0.752     Train Loss: 1.109\n",
      "Val Accuracy:   0.706       Val F1:     0.82     Val Loss:   0.803\n",
      "\n",
      "F1 EarlyStopping: 3/15\n",
      "\n",
      "Epoch [37/200]\n",
      "Train Accuracy: 0.763     Train Loss: 1.113\n",
      "Val Accuracy:   0.711       Val F1:     0.82     Val Loss:   0.800\n",
      "\n",
      "F1 EarlyStopping: 4/15\n",
      "\n",
      "Epoch [38/200]\n",
      "Train Accuracy: 0.759     Train Loss: 1.109\n",
      "Val Accuracy:   0.713       Val F1:     0.82     Val Loss:   0.813\n",
      "\n",
      "F1 EarlyStopping: 5/15\n",
      "\n",
      "Epoch [39/200]\n",
      "Train Accuracy: 0.763     Train Loss: 1.100\n",
      "Val Accuracy:   0.701       Val F1:     0.82     Val Loss:   0.851\n",
      "\n",
      "Epoch [40/200]\n",
      "Train Accuracy: 0.772     Train Loss: 1.086\n",
      "Val Accuracy:   0.714       Val F1:     0.82     Val Loss:   0.838\n",
      "\n",
      "Epoch [41/200]\n",
      "Train Accuracy: 0.767     Train Loss: 1.088\n",
      "Val Accuracy:   0.721       Val F1:     0.82     Val Loss:   0.814\n",
      "\n",
      "Epoch [42/200]\n",
      "Train Accuracy: 0.774     Train Loss: 1.074\n",
      "Val Accuracy:   0.720       Val F1:     0.83     Val Loss:   0.805\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [43/200]\n",
      "Train Accuracy: 0.772     Train Loss: 1.077\n",
      "Val Accuracy:   0.703       Val F1:     0.82     Val Loss:   0.844\n",
      "\n",
      "F1 EarlyStopping: 2/15\n",
      "\n",
      "Epoch [44/200]\n",
      "Train Accuracy: 0.776     Train Loss: 1.054\n",
      "Val Accuracy:   0.728       Val F1:     0.83     Val Loss:   0.801\n",
      "\n",
      "F1 EarlyStopping: 3/15\n",
      "\n",
      "Epoch [45/200]\n",
      "Train Accuracy: 0.787     Train Loss: 1.050\n",
      "Val Accuracy:   0.717       Val F1:     0.83     Val Loss:   0.841\n",
      "\n",
      "F1 EarlyStopping: 4/15\n",
      "\n",
      "Epoch [46/200]\n",
      "Train Accuracy: 0.775     Train Loss: 1.058\n",
      "Val Accuracy:   0.708       Val F1:     0.82     Val Loss:   0.822\n",
      "\n",
      "Epoch [47/200]\n",
      "Train Accuracy: 0.778     Train Loss: 1.049\n",
      "Val Accuracy:   0.731       Val F1:     0.83     Val Loss:   0.786\n",
      "\n",
      "Epoch [48/200]\n",
      "Train Accuracy: 0.802     Train Loss: 1.022\n",
      "Val Accuracy:   0.736       Val F1:     0.84     Val Loss:   0.786\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [49/200]\n",
      "Train Accuracy: 0.801     Train Loss: 1.028\n",
      "Val Accuracy:   0.713       Val F1:     0.83     Val Loss:   0.805\n",
      "\n",
      "F1 EarlyStopping: 2/15\n",
      "\n",
      "Epoch [50/200]\n",
      "Train Accuracy: 0.797     Train Loss: 1.028\n",
      "Val Accuracy:   0.727       Val F1:     0.83     Val Loss:   0.829\n",
      "\n",
      "F1 EarlyStopping: 3/15\n",
      "\n",
      "Epoch [51/200]\n",
      "Train Accuracy: 0.805     Train Loss: 1.004\n",
      "Val Accuracy:   0.716       Val F1:     0.82     Val Loss:   0.833\n",
      "\n",
      "F1 EarlyStopping: 4/15\n",
      "\n",
      "Epoch [52/200]\n",
      "Train Accuracy: 0.801     Train Loss: 1.007\n",
      "Val Accuracy:   0.718       Val F1:     0.82     Val Loss:   0.840\n",
      "\n",
      "F1 EarlyStopping: 5/15\n",
      "\n",
      "Epoch [53/200]\n",
      "Train Accuracy: 0.807     Train Loss: 1.015\n",
      "Val Accuracy:   0.732       Val F1:     0.84     Val Loss:   0.804\n",
      "\n",
      "F1 EarlyStopping: 6/15\n",
      "\n",
      "Epoch [54/200]\n",
      "Train Accuracy: 0.809     Train Loss: 0.994\n",
      "Val Accuracy:   0.725       Val F1:     0.83     Val Loss:   0.815\n",
      "\n",
      "F1 EarlyStopping: 7/15\n",
      "\n",
      "Epoch [55/200]\n",
      "Train Accuracy: 0.815     Train Loss: 0.979\n",
      "Val Accuracy:   0.717       Val F1:     0.83     Val Loss:   0.836\n",
      "\n",
      "F1 EarlyStopping: 8/15\n",
      "\n",
      "Epoch [56/200]\n",
      "Train Accuracy: 0.807     Train Loss: 0.995\n",
      "Val Accuracy:   0.728       Val F1:     0.83     Val Loss:   0.798\n",
      "\n",
      "F1 EarlyStopping: 9/15\n",
      "\n",
      "Epoch [57/200]\n",
      "Train Accuracy: 0.825     Train Loss: 0.974\n",
      "Val Accuracy:   0.730       Val F1:     0.83     Val Loss:   0.801\n",
      "\n",
      "F1 EarlyStopping: 10/15\n",
      "\n",
      "Epoch [58/200]\n",
      "Train Accuracy: 0.816     Train Loss: 0.973\n",
      "Val Accuracy:   0.726       Val F1:     0.83     Val Loss:   0.860\n",
      "\n",
      "F1 EarlyStopping: 11/15\n",
      "\n",
      "Epoch [59/200]\n",
      "Train Accuracy: 0.822     Train Loss: 0.966\n",
      "Val Accuracy:   0.725       Val F1:     0.83     Val Loss:   0.844\n",
      "\n",
      "F1 EarlyStopping: 12/15\n",
      "\n",
      "Epoch [60/200]\n",
      "Train Accuracy: 0.820     Train Loss: 0.965\n",
      "Val Accuracy:   0.725       Val F1:     0.83     Val Loss:   0.835\n",
      "\n",
      "F1 EarlyStopping: 13/15\n",
      "\n",
      "Epoch [61/200]\n",
      "Train Accuracy: 0.821     Train Loss: 0.965\n",
      "Val Accuracy:   0.730       Val F1:     0.83     Val Loss:   0.815\n",
      "\n",
      "Epoch [62/200]\n",
      "Train Accuracy: 0.828     Train Loss: 0.956\n",
      "Val Accuracy:   0.736       Val F1:     0.84     Val Loss:   0.827\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [63/200]\n",
      "Train Accuracy: 0.830     Train Loss: 0.949\n",
      "Val Accuracy:   0.723       Val F1:     0.84     Val Loss:   0.844\n",
      "\n",
      "F1 EarlyStopping: 2/15\n",
      "\n",
      "Epoch [64/200]\n",
      "Train Accuracy: 0.826     Train Loss: 0.948\n",
      "Val Accuracy:   0.720       Val F1:     0.83     Val Loss:   0.858\n",
      "\n",
      "F1 EarlyStopping: 3/15\n",
      "\n",
      "Epoch [65/200]\n",
      "Train Accuracy: 0.834     Train Loss: 0.944\n",
      "Val Accuracy:   0.731       Val F1:     0.83     Val Loss:   0.814\n",
      "\n",
      "Epoch [66/200]\n",
      "Train Accuracy: 0.840     Train Loss: 0.929\n",
      "Val Accuracy:   0.739       Val F1:     0.84     Val Loss:   0.805\n",
      "\n",
      "Epoch [67/200]\n",
      "Train Accuracy: 0.837     Train Loss: 0.931\n",
      "Val Accuracy:   0.736       Val F1:     0.84     Val Loss:   0.814\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [68/200]\n",
      "Train Accuracy: 0.834     Train Loss: 0.930\n",
      "Val Accuracy:   0.737       Val F1:     0.83     Val Loss:   0.792\n",
      "\n",
      "F1 EarlyStopping: 2/15\n",
      "\n",
      "Epoch [69/200]\n",
      "Train Accuracy: 0.841     Train Loss: 0.924\n",
      "Val Accuracy:   0.733       Val F1:     0.84     Val Loss:   0.826\n",
      "\n",
      "F1 EarlyStopping: 3/15\n",
      "\n",
      "Epoch [70/200]\n",
      "Train Accuracy: 0.842     Train Loss: 0.901\n",
      "Val Accuracy:   0.731       Val F1:     0.84     Val Loss:   0.816\n",
      "\n",
      "F1 EarlyStopping: 4/15\n",
      "\n",
      "Epoch [71/200]\n",
      "Train Accuracy: 0.843     Train Loss: 0.905\n",
      "Val Accuracy:   0.730       Val F1:     0.83     Val Loss:   0.848\n",
      "\n",
      "F1 EarlyStopping: 5/15\n",
      "\n",
      "Epoch [72/200]\n",
      "Train Accuracy: 0.844     Train Loss: 0.916\n",
      "Val Accuracy:   0.726       Val F1:     0.84     Val Loss:   0.862\n",
      "\n",
      "F1 EarlyStopping: 6/15\n",
      "\n",
      "Epoch [73/200]\n",
      "Train Accuracy: 0.842     Train Loss: 0.909\n",
      "Val Accuracy:   0.735       Val F1:     0.84     Val Loss:   0.832\n",
      "\n",
      "F1 EarlyStopping: 7/15\n",
      "\n",
      "Epoch [74/200]\n",
      "Train Accuracy: 0.842     Train Loss: 0.928\n",
      "Val Accuracy:   0.727       Val F1:     0.83     Val Loss:   0.883\n",
      "\n",
      "Epoch [75/200]\n",
      "Train Accuracy: 0.848     Train Loss: 0.900\n",
      "Val Accuracy:   0.746       Val F1:     0.84     Val Loss:   0.813\n",
      "\n",
      "Epoch [76/200]\n",
      "Train Accuracy: 0.854     Train Loss: 0.891\n",
      "Val Accuracy:   0.750       Val F1:     0.85     Val Loss:   0.779\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [77/200]\n",
      "Train Accuracy: 0.854     Train Loss: 0.898\n",
      "Val Accuracy:   0.739       Val F1:     0.84     Val Loss:   0.890\n",
      "\n",
      "Epoch [78/200]\n",
      "Train Accuracy: 0.853     Train Loss: 0.876\n",
      "Val Accuracy:   0.749       Val F1:     0.85     Val Loss:   0.815\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [79/200]\n",
      "Train Accuracy: 0.854     Train Loss: 0.887\n",
      "Val Accuracy:   0.742       Val F1:     0.84     Val Loss:   0.860\n",
      "\n",
      "F1 EarlyStopping: 2/15\n",
      "\n",
      "Epoch [80/200]\n",
      "Train Accuracy: 0.852     Train Loss: 0.893\n",
      "Val Accuracy:   0.739       Val F1:     0.84     Val Loss:   0.856\n",
      "\n",
      "F1 EarlyStopping: 3/15\n",
      "\n",
      "Epoch [81/200]\n",
      "Train Accuracy: 0.859     Train Loss: 0.876\n",
      "Val Accuracy:   0.748       Val F1:     0.84     Val Loss:   0.804\n",
      "\n",
      "F1 EarlyStopping: 4/15\n",
      "\n",
      "Epoch [82/200]\n",
      "Train Accuracy: 0.856     Train Loss: 0.868\n",
      "Val Accuracy:   0.749       Val F1:     0.85     Val Loss:   0.846\n",
      "\n",
      "F1 EarlyStopping: 5/15\n",
      "\n",
      "Epoch [83/200]\n",
      "Train Accuracy: 0.854     Train Loss: 0.879\n",
      "Val Accuracy:   0.741       Val F1:     0.84     Val Loss:   0.846\n",
      "\n",
      "Epoch [84/200]\n",
      "Train Accuracy: 0.849     Train Loss: 0.875\n",
      "Val Accuracy:   0.754       Val F1:     0.85     Val Loss:   0.783\n",
      "\n",
      "Epoch [85/200]\n",
      "Train Accuracy: 0.865     Train Loss: 0.866\n",
      "Val Accuracy:   0.758       Val F1:     0.85     Val Loss:   0.800\n",
      "\n",
      "F1 EarlyStopping: 1/15\n",
      "\n",
      "Epoch [86/200]\n",
      "Train Accuracy: 0.859     Train Loss: 0.872\n",
      "Val Accuracy:   0.739       Val F1:     0.84     Val Loss:   0.835\n",
      "\n",
      "F1 EarlyStopping: 2/15\n",
      "\n",
      "Epoch [87/200]\n",
      "Train Accuracy: 0.857     Train Loss: 0.867\n",
      "Val Accuracy:   0.752       Val F1:     0.85     Val Loss:   0.829\n",
      "\n",
      "F1 EarlyStopping: 3/15\n",
      "\n",
      "Epoch [88/200]\n",
      "Train Accuracy: 0.862     Train Loss: 0.867\n",
      "Val Accuracy:   0.746       Val F1:     0.84     Val Loss:   0.842\n",
      "\n",
      "F1 EarlyStopping: 4/15\n",
      "\n",
      "Epoch [89/200]\n",
      "Train Accuracy: 0.855     Train Loss: 0.870\n",
      "Val Accuracy:   0.735       Val F1:     0.83     Val Loss:   0.830\n",
      "\n",
      "F1 EarlyStopping: 5/15\n",
      "\n",
      "Epoch [90/200]\n",
      "Train Accuracy: 0.867     Train Loss: 0.847\n",
      "Val Accuracy:   0.738       Val F1:     0.84     Val Loss:   0.854\n",
      "\n",
      "F1 EarlyStopping: 6/15\n",
      "\n",
      "Epoch [91/200]\n",
      "Train Accuracy: 0.864     Train Loss: 0.852\n",
      "Val Accuracy:   0.749       Val F1:     0.84     Val Loss:   0.835\n",
      "\n",
      "F1 EarlyStopping: 7/15\n",
      "\n",
      "Epoch [92/200]\n",
      "Train Accuracy: 0.863     Train Loss: 0.850\n",
      "Val Accuracy:   0.737       Val F1:     0.84     Val Loss:   0.867\n",
      "\n",
      "F1 EarlyStopping: 8/15\n",
      "\n",
      "Epoch [93/200]\n",
      "Train Accuracy: 0.870     Train Loss: 0.843\n",
      "Val Accuracy:   0.734       Val F1:     0.84     Val Loss:   0.869\n",
      "\n",
      "F1 EarlyStopping: 9/15\n",
      "\n",
      "Epoch [94/200]\n",
      "Train Accuracy: 0.878     Train Loss: 0.833\n",
      "Val Accuracy:   0.741       Val F1:     0.84     Val Loss:   0.811\n",
      "\n",
      "F1 EarlyStopping: 10/15\n",
      "\n",
      "Epoch [95/200]\n",
      "Train Accuracy: 0.880     Train Loss: 0.839\n",
      "Val Accuracy:   0.739       Val F1:     0.84     Val Loss:   0.847\n",
      "\n",
      "F1 EarlyStopping: 11/15\n",
      "\n",
      "Epoch [96/200]\n",
      "Train Accuracy: 0.875     Train Loss: 0.833\n",
      "Val Accuracy:   0.746       Val F1:     0.84     Val Loss:   0.837\n",
      "\n",
      "F1 EarlyStopping: 12/15\n",
      "\n",
      "Epoch [97/200]\n",
      "Train Accuracy: 0.871     Train Loss: 0.837\n",
      "Val Accuracy:   0.752       Val F1:     0.85     Val Loss:   0.844\n",
      "\n",
      "F1 EarlyStopping: 13/15\n",
      "\n",
      "Epoch [98/200]\n",
      "Train Accuracy: 0.871     Train Loss: 0.844\n",
      "Val Accuracy:   0.744       Val F1:     0.84     Val Loss:   0.857\n",
      "\n",
      "F1 EarlyStopping: 14/15\n",
      "\n",
      "Epoch [99/200]\n",
      "Train Accuracy: 0.876     Train Loss: 0.843\n",
      "Val Accuracy:   0.749       Val F1:     0.84     Val Loss:   0.868\n",
      "\n",
      "F1 EarlyStopping: 15/15\n",
      "\n",
      "\n",
      "Early stopping triggered at epoch 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopper = F1EarlyStopping(PATIENCE, verbose=True)\n",
    "\n",
    "train_history = train_eval(\n",
    "    EPOCHS, \n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer, \n",
    "    class_weights, \n",
    "    label_encoder, \n",
    "    scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d463c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T04:25:04.577696Z",
     "iopub.status.busy": "2025-06-21T04:25:04.577435Z",
     "iopub.status.idle": "2025-06-21T04:25:04.586980Z",
     "shell.execute_reply": "2025-06-21T04:25:04.586412Z"
    },
    "papermill": {
     "duration": 0.019552,
     "end_time": "2025-06-21T04:25:04.587983",
     "exception": false,
     "start_time": "2025-06-21T04:25:04.568431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_history.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_history, \"train_history.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "sourceId": 245998721,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 246591810,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3587.859252,
   "end_time": "2025-06-21T04:25:07.561282",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-21T03:25:19.702030",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
